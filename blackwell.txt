root@9fabc796a5ec:/workspace/dark.rl# uv run python ./integration_test_streams_unsloth.py 
Using CPython 3.11.9
Creating virtual environment at: .venv
      Built dark-rl @ file:///workspace/dark.rl
  × Failed to build `flash-attn==2.8.0.post2`
  ├─▶ The build backend returned an error
  ╰─▶ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)

      [stderr]
      Traceback (most recent call last):
        File "<string>", line 14, in <module>
        File "/root/.cache/uv/builds-v0/.tmpD8ofww/lib/python3.11/site-packages/setuptools/build_meta.py", line 331, in
      get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=[])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/root/.cache/uv/builds-v0/.tmpD8ofww/lib/python3.11/site-packages/setuptools/build_meta.py", line 301, in
      _get_build_requires
          self.run_setup()
        File "/root/.cache/uv/builds-v0/.tmpD8ofww/lib/python3.11/site-packages/setuptools/build_meta.py", line 512, in run_setup
          super().run_setup(setup_script=setup_script)
        File "/root/.cache/uv/builds-v0/.tmpD8ofww/lib/python3.11/site-packages/setuptools/build_meta.py", line 317, in run_setup
          exec(code, locals())
        File "<string>", line 22, in <module>
      ModuleNotFoundError: No module named 'torch'

      hint: This error likely indicates that `flash-attn@2.8.0.post2` depends on `torch`, but doesn't declare it as a build dependency.
      If `flash-attn` is a first-party package, consider adding `torch` to its `build-system.requires`. Otherwise, `uv pip install
      torch` into the environment and re-run with `--no-build-isolation`.
  help: `flash-attn` (v2.8.0.post2) was included because `dark-rl` (v0.1.0) depends on `flash-attn`
root@9fabc796a5ec:/workspace/dark.rl# source $HOME/.local/bin/env^C
root@9fabc796a5ec:/workspace/dark.rl# uv run pip install flash-attn --no-build-isolation
  × Failed to build `flash-attn==2.8.0.post2`
  ├─▶ The build backend returned an error
  ╰─▶ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)

      [stderr]
      Traceback (most recent call last):
        File "<string>", line 14, in <module>
        File "/root/.cache/uv/builds-v0/.tmpiZ5foz/lib/python3.11/site-packages/setuptools/build_meta.py", line 331, in
      get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=[])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/root/.cache/uv/builds-v0/.tmpiZ5foz/lib/python3.11/site-packages/setuptools/build_meta.py", line 301, in
      _get_build_requires
          self.run_setup()
        File "/root/.cache/uv/builds-v0/.tmpiZ5foz/lib/python3.11/site-packages/setuptools/build_meta.py", line 512, in run_setup
          super().run_setup(setup_script=setup_script)
        File "/root/.cache/uv/builds-v0/.tmpiZ5foz/lib/python3.11/site-packages/setuptools/build_meta.py", line 317, in run_setup
          exec(code, locals())
        File "<string>", line 22, in <module>
      ModuleNotFoundError: No module named 'torch'

      hint: This error likely indicates that `flash-attn@2.8.0.post2` depends on `torch`, but doesn't declare it as a build dependency.
      If `flash-attn` is a first-party package, consider adding `torch` to its `build-system.requires`. Otherwise, `uv pip install
      torch` into the environment and re-run with `--no-build-isolation`.
  help: `flash-attn` (v2.8.0.post2) was included because `dark-rl` (v0.1.0) depends on `flash-attn`
root@9fabc796a5ec:/workspace/dark.rl# uv run pip install torch
  × Failed to build `flash-attn==2.8.0.post2`
  ├─▶ The build backend returned an error
  ╰─▶ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)

      [stderr]
      Traceback (most recent call last):
        File "<string>", line 14, in <module>
        File "/root/.cache/uv/builds-v0/.tmpGffUg8/lib/python3.11/site-packages/setuptools/build_meta.py", line 331, in
      get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=[])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/root/.cache/uv/builds-v0/.tmpGffUg8/lib/python3.11/site-packages/setuptools/build_meta.py", line 301, in
      _get_build_requires
          self.run_setup()
        File "/root/.cache/uv/builds-v0/.tmpGffUg8/lib/python3.11/site-packages/setuptools/build_meta.py", line 512, in run_setup
          super().run_setup(setup_script=setup_script)
        File "/root/.cache/uv/builds-v0/.tmpGffUg8/lib/python3.11/site-packages/setuptools/build_meta.py", line 317, in run_setup
          exec(code, locals())
        File "<string>", line 22, in <module>
      ModuleNotFoundError: No module named 'torch'

      hint: This error likely indicates that `flash-attn@2.8.0.post2` depends on `torch`, but doesn't declare it as a build dependency.
      If `flash-attn` is a first-party package, consider adding `torch` to its `build-system.requires`. Otherwise, `uv pip install
      torch` into the environment and re-run with `--no-build-isolation`.
  help: `flash-attn` (v2.8.0.post2) was included because `dark-rl` (v0.1.0) depends on `flash-attn`
root@9fabc796a5ec:/workspace/dark.rl# uv sync
Resolved 55 packages in 14ms
  × Failed to build `flash-attn==2.8.0.post2`
  ├─▶ The build backend returned an error
  ╰─▶ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)

      [stderr]
      Traceback (most recent call last):
        File "<string>", line 14, in <module>
        File "/root/.cache/uv/builds-v0/.tmpKCN2r6/lib/python3.11/site-packages/setuptools/build_meta.py", line 331, in
      get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=[])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/root/.cache/uv/builds-v0/.tmpKCN2r6/lib/python3.11/site-packages/setuptools/build_meta.py", line 301, in
      _get_build_requires
          self.run_setup()
        File "/root/.cache/uv/builds-v0/.tmpKCN2r6/lib/python3.11/site-packages/setuptools/build_meta.py", line 512, in run_setup
          super().run_setup(setup_script=setup_script)
        File "/root/.cache/uv/builds-v0/.tmpKCN2r6/lib/python3.11/site-packages/setuptools/build_meta.py", line 317, in run_setup
          exec(code, locals())
        File "<string>", line 22, in <module>
      ModuleNotFoundError: No module named 'torch'

      hint: This error likely indicates that `flash-attn@2.8.0.post2` depends on `torch`, but doesn't declare it as a build dependency.
      If `flash-attn` is a first-party package, consider adding `torch` to its `build-system.requires`. Otherwise, `uv pip install
      torch` into the environment and re-run with `--no-build-isolation`.
  help: `flash-attn` (v2.8.0.post2) was included because `dark-rl` (v0.1.0) depends on `flash-attn`
root@9fabc796a5ec:/workspace/dark.rl# uv pip install torch
Resolved 25 packages in 1.22s
Prepared 10 packages in 58.54s
░░░░░░░░░░░░░░░░░░░░ [0/25] Installing wheels...                                                                                         warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 25 packages in 3m 30s
 + filelock==3.18.0
 + fsspec==2025.5.1
 + jinja2==3.1.6
 + markupsafe==3.0.2
 + mpmath==1.3.0
 + networkx==3.5
 + nvidia-cublas-cu12==12.6.4.1
 + nvidia-cuda-cupti-cu12==12.6.80
 + nvidia-cuda-nvrtc-cu12==12.6.77
 + nvidia-cuda-runtime-cu12==12.6.77
 + nvidia-cudnn-cu12==9.5.1.17
 + nvidia-cufft-cu12==11.3.0.4
 + nvidia-cufile-cu12==1.11.1.6
 + nvidia-curand-cu12==10.3.7.77
 + nvidia-cusolver-cu12==11.7.1.2
 + nvidia-cusparse-cu12==12.5.4.2
 + nvidia-cusparselt-cu12==0.6.3
 + nvidia-nccl-cu12==2.26.2
 + nvidia-nvjitlink-cu12==12.6.85
 + nvidia-nvtx-cu12==12.6.77
 + setuptools==80.9.0
 + sympy==1.14.0
 + torch==2.7.1
 + triton==3.3.1
 + typing-extensions==4.14.0
root@9fabc796a5ec:/workspace/dark.rl# uv run pip install flash-attn --no-build-isolation
  × Failed to build `flash-attn==2.8.0.post2`
  ├─▶ The build backend returned an error
  ╰─▶ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)

      [stderr]
      Traceback (most recent call last):
        File "<string>", line 14, in <module>
        File "/root/.cache/uv/builds-v0/.tmpYVqL0S/lib/python3.11/site-packages/setuptools/build_meta.py", line 331, in
      get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=[])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/root/.cache/uv/builds-v0/.tmpYVqL0S/lib/python3.11/site-packages/setuptools/build_meta.py", line 301, in
      _get_build_requires
          self.run_setup()
        File "/root/.cache/uv/builds-v0/.tmpYVqL0S/lib/python3.11/site-packages/setuptools/build_meta.py", line 512, in run_setup
          super().run_setup(setup_script=setup_script)
        File "/root/.cache/uv/builds-v0/.tmpYVqL0S/lib/python3.11/site-packages/setuptools/build_meta.py", line 317, in run_setup
          exec(code, locals())
        File "<string>", line 22, in <module>
      ModuleNotFoundError: No module named 'torch'

      hint: This error likely indicates that `flash-attn@2.8.0.post2` depends on `torch`, but doesn't declare it as a build dependency.
      If `flash-attn` is a first-party package, consider adding `torch` to its `build-system.requires`. Otherwise, `uv pip install
      torch` into the environment and re-run with `--no-build-isolation`.
  help: `flash-attn` (v2.8.0.post2) was included because `dark-rl` (v0.1.0) depends on `flash-attn`
root@9fabc796a5ec:/workspace/dark.rl# uv pip install flash-attn --no-build-isolation
Resolved 27 packages in 575ms
      Built flash-attn==2.8.0.post2
Prepared 1 package in 1m 09s
░░░░░░░░░░░░░░░░░░░░ [0/2] Installing wheels...                                                                                          warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 2 packages in 13.70s
 + einops==0.8.1
 + flash-attn==2.8.0.post2
root@9fabc796a5ec:/workspace/dark.rl# uv run python ./integration_test_streams_unsloth.py 
░░░░░░░░░░░░░░░░░░░░ [0/19] Installing wheels...                                                                                         warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 19 packages in 33.60s
/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:287: UserWarning: 
NVIDIA B200 with CUDA capability sm_100 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.
If you want to use the NVIDIA B200 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(
Using Adam 8-bit optimizer: True
--- LoRA CPU offloading is ENABLED ---
README.md: 100%|████████████████████████████████████████████████████████████████████████████████████| 16.7k/16.7k [00:00<00:00, 22.4MB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 728/728 [00:00<00:00, 2.47MB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████| 1.57k/1.57k [00:00<00:00, 4.07MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████| 239/239 [00:00<00:00, 749kB/s]
model.safetensors.index.json: 100%|█████████████████████████████████████████████████████████████████| 32.9k/32.9k [00:00<00:00, 45.0MB/s]
merges.txt: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.67M/1.67M [00:00<00:00, 3.46MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████| 9.73k/9.73k [00:00<00:00, 22.7MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████| 11.4M/11.4M [00:01<00:00, 10.6MB/s]
vocab.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 2.78M/2.78M [00:00<00:00, 4.15MB/s]
model-00003-of-00005.safetensors: 100%|██████████████████████████████████████████████████████████████| 3.96G/3.96G [00:33<00:00, 117MB/s]
model-00001-of-00005.safetensors: 100%|██████████████████████████████████████████████████████████████| 4.00G/4.00G [00:34<00:00, 116MB/s]
model-00005-of-00005.safetensors: 100%|█████████████████████████████████████████████████████████████| 1.24G/1.24G [00:34<00:00, 36.1MB/s]
model-00004-of-00005.safetensors: 100%|█████████████████████████████████████████████████████████████| 3.19G/3.19G [00:34<00:00, 92.7MB/s]
model-00002-of-00005.safetensors: 100%|██████████████████████████████████████████████████████████████| 3.99G/3.99G [00:34<00:00, 114MB/s]
Fetching 14 files: 100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:35<00:00,  2.53s/it]
Traceback (most recent call last):100%|██████████████████████████████████████████████████████████████| 3.99G/3.99G [00:34<00:00, 183MB/s]
  File "/workspace/dark.rl/./integration_test_streams_unsloth.py", line 506, in <module>
    asyncio.run(main()) etensors:  96%|███████████████████████████████████████████████████████████▍  | 3.05G/3.19G [00:34<00:01, 108MB/s]
    ^^^^^^^^^^^^^^^^^^^fetensors: 100%|██████████████████████████████████████████████████████████████| 3.19G/3.19G [00:34<00:00, 159MB/s]
  File "/root/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/./integration_test_streams_unsloth.py", line 328, in main
    llm = LLM(MODEL_PATH, enforce_eager=True, lora_rank=LORA_RANK, lora_alpha=LORA_ALPHA)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/src/dark/engine/llm_engine.py", line 67, in __init__
    self.model_runner = ModelRunner(config)
                        ^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/src/dark/engine/model_runner.py", line 24, in __init__
    self.model = Qwen3ForCausalLM(
                 ^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/src/dark/models/qwen3.py", line 204, in __init__
    self.model = Qwen3Model(config, lora_rank=lora_rank, lora_alpha=lora_alpha)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/src/dark/models/qwen3.py", line 172, in __init__
    self.embed_tokens = nn.Embedding(self.config.vocab_size, self.config.hidden_size, self.config.pad_token_id)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 170, in __init__
    self.reset_parameters()
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 181, in reset_parameters
    init.normal_(self.weight)
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/nn/init.py", line 190, in normal_
    return torch.overrides.handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/overrides.py", line 1721, in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/nn/init.py", line 193, in normal_
    return _no_grad_normal_(tensor, mean, std, generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dark.rl/.venv/lib/python3.11/site-packages/torch/nn/init.py", line 22, in _no_grad_normal_
    return tensor.normal_(mean, std, generator=generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

root@9fabc796a5ec:/workspace/dark.rl# nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Fri_Feb_21_20:23:50_PST_2025
Cuda compilation tools, release 12.8, V12.8.93
Build cuda_12.8.r12.8/compiler.35583870_0

root@9fabc796a5ec:/workspace/dark.rl# uv pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/